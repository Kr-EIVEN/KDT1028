# Cell 0: ê³µí†µ
import os, json, math, random, re
from typing import List, Tuple, Dict

import numpy as np
from PIL import Image
from tqdm import tqdm

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset, random_split
from torchvision import models, transforms, datasets

import clip

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)

ROOT = "."
DATA_ROOT = os.path.join(ROOT, "data")
RUNS_ROOT = os.path.join(ROOT, "runs_ft")
os.makedirs(RUNS_ROOT, exist_ok=True)

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
BATCH_SIZE = 32
NUM_WORKERS = 0
VAL_RATIO = 0.15
TEST_RATIO = 0.15

# ResNet í™•ì¥ í•™ìŠµ
EPOCHS_OBJ = 8
LR_OBJ = 3e-4
WD_OBJ = 1e-4

# CLIP ë¯¸ì„¸ì¡°ì • (ëŒ€ë¹„í•™ìŠµ)
EPOCHS_CLIP = 6
LR_CLIP = 5e-6     # CLIPì€ ì‘ì€ lr ê¶Œì¥ (í•„ìš”ì‹œ 1e-5 ~ 5e-6 ì‚¬ì´ íƒìƒ‰)
WD_CLIP = 0.0

# ì¶”ë¡  ê·œì¹™(ì´ì „ ìš”êµ¬ëŒ€ë¡œ ìœ ì§€)
OBJECT_THRESHOLD = 0.20    # ê°ì²´: 0.5 ì´ìƒ ëª¨ë‘
TOPK_SCENE = 2
TOPK_MOOD  = 2

def to_hashtag(s: str) -> str:
    return "#" + s.strip().replace(" ", "_")
# Cell 1: Transform / Split / Helper
def tfms_train(size=224):
    return transforms.Compose([
        transforms.Resize((size, size)),
        transforms.RandomHorizontalFlip(),
        transforms.ColorJitter(0.2,0.2,0.2,0.1),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),
    ])

def tfms_eval(size=224):
    return transforms.Compose([
        transforms.Resize((size, size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),
    ])

def build_splits(ds, val_ratio=VAL_RATIO, test_ratio=TEST_RATIO):
    n = len(ds)
    n_val = int(n * val_ratio)
    n_test = int(n * test_ratio)
    n_train = n - n_val - n_test
    return random_split(ds, [n_train, n_val, n_test], generator=torch.Generator().manual_seed(SEED))

def clean_caption_from_filename(name: str) -> str:
    # íŒŒì¼ëª… -> ìº¡ì…˜: í™•ì¥ì ì œê±°, ì–¸ë”ìŠ¤ì½”ì–´/ëŒ€ì‹œë¥¼ ê³µë°±ìœ¼ë¡œ
    base = os.path.splitext(os.path.basename(name))[0]
    base = re.sub(r'[_\-]+', ' ', base)
    return base.strip()
# Cell 2: ResNet í™•ì¥ ë¶„ë¥˜ê¸° (ê¸°ì¡´ 1000 + ìƒˆ í´ë˜ìŠ¤)
def load_imagenet_resnet50():
    w = models.ResNet50_Weights.IMAGENET1K_V1
    base = models.resnet50(weights=w)
    base.eval()
    imagenet_classes = w.meta["categories"]  # ê¸¸ì´ 1000
    return base, imagenet_classes

def build_extended_fc(old_fc: nn.Linear, num_new: int) -> nn.Linear:
    in_dim = old_fc.in_features
    out_old = old_fc.out_features
    out_new = out_old + num_new
    new_fc = nn.Linear(in_dim, out_new)
    # ê¸°ì¡´ ê°€ì¤‘ì¹˜/ë°”ì´ì–´ìŠ¤ ë³µì‚¬
    with torch.no_grad():
        new_fc.weight[:out_old] = old_fc.weight
        new_fc.bias[:out_old]   = old_fc.bias
        # ë‚˜ë¨¸ì§€ ìƒˆ í–‰ì€ Kaiming/Zero ì´ˆê¸°í™”
        nn.init.kaiming_normal_(new_fc.weight[out_old:])
        nn.init.constant_(new_fc.bias[out_old:], 0.0)
    return new_fc

def map_ext_classes(object_root: str, imagenet_classes: List[str]):
    """
    object_ext í´ë”ì˜ í´ë˜ìŠ¤ëª…ì„ í™•ì¸:
    - í´ë”ëª…ì´ ImageNet í´ë˜ìŠ¤ëª…ê³¼ 'ì™„ì „ ì¼ì¹˜'í•˜ë©´ old í´ë˜ìŠ¤ë¡œ ê°„ì£¼(ê°•í™”í•™ìŠµ ëŒ€ìƒ)
    - ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ new í´ë˜ìŠ¤ë¡œ ê°„ì£¼(í™•ì¥)
    """
    # ImageFolderë¡œ í´ë˜ìŠ¤ëª… í™•ë³´
    ds_tmp = datasets.ImageFolder(object_root, transform=tfms_train())
    custom_classes = ds_tmp.classes  # í´ë”ëª… ì‚¬ì „ìˆœ
    # ë§¤í•‘
    name_to_idx_imagenet = {name: i for i, name in enumerate(imagenet_classes)}
    old_indices, new_names = [], []
    for cname in custom_classes:
        if cname in name_to_idx_imagenet:
            old_indices.append(name_to_idx_imagenet[cname])
        else:
            new_names.append(cname)
    return old_indices, new_names, custom_classes

def make_extended_loaders(object_root: str, size=224):
    return build_splits(datasets.ImageFolder(object_root, transform=tfms_train(size)))
# Cell 3: ResNet í™•ì¥ í•™ìŠµ
def train_resnet_extended(object_root: str, out_dir: str, epochs=EPOCHS_OBJ, lr=LR_OBJ, wd=WD_OBJ):
    os.makedirs(out_dir, exist_ok=True)
    base, imagenet_classes = load_imagenet_resnet50()
    base = base.to(DEVICE).train()

    # ì–´ë–¤ í´ë˜ìŠ¤ê°€ old(ê¸°ì¡´ 1000)ì´ê³ , ì–´ë–¤ ê²Œ newì¸ì§€ íŒë³„
    old_indices, new_names, custom_classes = map_ext_classes(object_root, imagenet_classes)
    num_new = len(new_names)
    print(f"Existing(imagenet) matched: {len(old_indices)} | New classes to add: {num_new}")

    # í™•ì¥ FC êµ¬ì„±
    base.fc = build_extended_fc(base.fc, num_new).to(DEVICE)

    # í™•ì¥ëœ í´ë˜ìŠ¤ ì´ë¦„ í…Œì´ë¸”(ê¸¸ì´ 1000+num_new)
    ext_classnames = list(imagenet_classes) + new_names

    # ìš°ë¦¬ ë°ì´í„°ì…‹ ë¡œë”
    train_ds, val_ds, test_ds = make_extended_loaders(object_root)
    # eval transform ì ìš©
    val_ds.dataset.transform = tfms_eval()
    test_ds.dataset.transform = tfms_eval()
    tr = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS)
    va = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)
    te = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)

    # ë¼ë²¨ ë§¤í•‘: ì»¤ìŠ¤í…€ í´ë˜ìŠ¤ â†’ í™•ì¥ ì¸ë±ìŠ¤
    # - í´ë”ëª…ì´ ImageNetì— ìˆìœ¼ë©´ â†’ ê·¸ index ì‚¬ìš©
    # - ì—†ìœ¼ë©´ â†’ 1000 + ìƒˆ í´ë˜ìŠ¤ì˜ index (custom_classes ìˆœì„œ ê¸°ë°˜ ë§¤í•‘ í•„ìš”)
    name_to_idx_imagenet = {n:i for i,n in enumerate(imagenet_classes)}
    new_name_to_ext = {n: 1000 + i for i,n in enumerate(new_names)}
    # ImageFolderì˜ class_to_idxë¡œ ë°°ì¹˜ì˜ yë¥¼ ext ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” ë˜í¼
    def to_ext_idx(y: torch.Tensor):
        # yëŠ” ImageFolder ê¸°ì¤€ ì¸ë±ìŠ¤ â†’ í•´ë‹¹ ì´ë¦„ì„ ì°¾ì•„ extë¡œ ë³€í™˜
        inv = {v:k for k,v in train_ds.dataset.class_to_idx.items()}
        out = y.clone()
        for j in range(y.shape[0]):
            cname = inv[int(y[j].item())]
            if cname in name_to_idx_imagenet:
                out[j] = name_to_idx_imagenet[cname]
            else:
                out[j] = new_name_to_ext[cname]
        return out.to(DEVICE)

    # ì˜µí‹°ë§ˆì´ì € (ì „ì¸µ ë¯¸ì„¸ì¡°ì •)
    opt = torch.optim.AdamW(base.parameters(), lr=lr, weight_decay=wd)
    criterion = nn.CrossEntropyLoss()

    def epoch_loop(loader, train_flag=True):
        base.train() if train_flag else base.eval()
        tot, corr, n = 0.0, 0, 0
        for x, y in loader:
            x = x.to(DEVICE)
            y = to_ext_idx(y)  # â† í™•ì¥ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
            if train_flag: opt.zero_grad()
            logits = base(x)
            loss = criterion(logits, y)
            if train_flag:
                loss.backward(); opt.step()
            tot += loss.item()*x.size(0)
            pred = logits.argmax(1)
            corr += (pred==y).sum().item()
            n += x.size(0)
        return tot/n, corr/n

    best_acc, ckpt_path = -1, os.path.join(out_dir, "resnet50_ext.pt")
    for ep in range(1, epochs+1):
        tl, ta = epoch_loop(tr, True)
        vl, va_acc = epoch_loop(va, False)
        print(f"[EXT][{ep}/{epochs}] train {tl:.4f}/{ta:.3f} | val {vl:.4f}/{va_acc:.3f}")
        if va_acc > best_acc:
            best_acc = va_acc
            torch.save({
                "state_dict": base.state_dict(),
                "ext_classnames": ext_classnames,  # ê¸¸ì´ 1000+Î±
            }, ckpt_path)

    # test
    tl, ta = epoch_loop(te, False)
    print(f"[EXT][TEST] loss {tl:.4f} acc {ta:.3f}")
    return ckpt_path, ext_classnames
# Cell 4: CLIP ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ëŒ€ë¹„í•™ìŠµ ë¯¸ì„¸ì¡°ì •
class CaptionImageDataset(Dataset):
    def __init__(self, folder: str, preprocess, tokenizer):
        self.paths = []
        for f in os.listdir(folder):
            p = os.path.join(folder, f)
            if os.path.isfile(p) and os.path.splitext(p)[1].lower() in (".jpg",".jpeg",".png",".webp",".bmp"):
                self.paths.append(p)
        self.paths.sort()
        self.preprocess = preprocess
        self.tokenizer = tokenizer

    def __len__(self): return len(self.paths)
    def __getitem__(self, idx):
        p = self.paths[idx]
        img = Image.open(p).convert("RGB")
        x = self.preprocess(img)
        caption = clean_caption_from_filename(p)
        return x, caption

def clip_contrastive_train(folder: str, out_dir: str, epochs=EPOCHS_CLIP, lr=LR_CLIP, wd=WD_CLIP):
    os.makedirs(out_dir, exist_ok=True)
    model, preprocess = clip.load("ViT-B/32", device=DEVICE)
    ds_full = CaptionImageDataset(folder, preprocess, clip.tokenize)
    # train/val split
    n = len(ds_full)
    n_val = max(1, int(n*VAL_RATIO))
    n_train = n - n_val
    tr_ds, va_ds = random_split(ds_full, [n_train, n_val], generator=torch.Generator().manual_seed(SEED))
    tr = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS)
    va = DataLoader(va_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)

    # ì˜µí‹°ë§ˆì´ì €: ì „ì²´ íŒŒë¼ë¯¸í„°(ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ì¸ì½”ë” í¬í•¨) ë¯¸ì„¸ì¡°ì • (ì›í•˜ë©´ ì¼ë¶€ë§Œ í’€ ìˆ˜ë„ ìˆìŒ)
    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)

    def run_epoch(loader, train_flag=True):
        model.train() if train_flag else model.eval()
        tot, n = 0.0, 0
        for imgs, caps in loader:
            imgs = imgs.to(DEVICE)
            tokens = clip.tokenize(caps, truncate=True).to(DEVICE)
            if train_flag: opt.zero_grad()
            logits_per_image, logits_per_text = model(imgs, tokens)
            # CLIPì˜ ê¸°ë³¸ ëŒ€ë¹„í•™ìŠµ ì†ì‹¤ = ì´ë¯¸ì§€â†’í…ìŠ¤íŠ¸, í…ìŠ¤íŠ¸â†’ì´ë¯¸ì§€ CE í‰ê· 
            labels = torch.arange(len(imgs), device=DEVICE)
            loss_i = nn.CrossEntropyLoss()(logits_per_image, labels)
            loss_t = nn.CrossEntropyLoss()(logits_per_text, labels)
            loss = (loss_i + loss_t) / 2
            if train_flag:
                loss.backward(); opt.step()
            tot += loss.item()*imgs.size(0); n += imgs.size(0)
        return tot/max(1,n)

    best_val, ckpt_path = 1e9, os.path.join(out_dir, "clip_vitb32_finetuned.pt")
    for ep in range(1, epochs+1):
        tl = run_epoch(tr, True)
        vl = run_epoch(va, False)
        print(f"[CLIP-FT][{ep}/{epochs}] train loss {tl:.4f} | val loss {vl:.4f}")
        if vl < best_val:
            best_val = vl
            torch.save({"clip_name":"ViT-B/32","state_dict":model.state_dict()}, ckpt_path)

    return ckpt_path
# Cell 5: í•™ìŠµ ì‹¤í–‰ ìŠ¤ìœ„ì¹˜
OBJ_EXT_ROOT = os.path.join(DATA_ROOT, "object_ext")
CLIP_PAIRS   = os.path.join(DATA_ROOT, "clip_pairs")

OUT_OBJ_EXT  = os.path.join(RUNS_ROOT, "object_ext")
OUT_CLIP_FT  = os.path.join(RUNS_ROOT, "clip_finetune")

DO_TRAIN_RESNET_EXT = False
DO_TRAIN_CLIP_FT    = False

if DO_TRAIN_RESNET_EXT:
    path_obj_ckpt, ext_names = train_resnet_extended(OBJ_EXT_ROOT, OUT_OBJ_EXT)

if DO_TRAIN_CLIP_FT:
    path_clip_ckpt = clip_contrastive_train(CLIP_PAIRS, OUT_CLIP_FT)
# Cell 6: ì¶”ë¡  ìœ í‹¸ (ì²´í¬í¬ì¸íŠ¸ ì—†ìœ¼ë©´ ìë™ í´ë°±)
import matplotlib.pyplot as plt

# ========== ResNet (í™•ì¥) ==========
def load_resnet_ext_ckpt_or_fallback(out_dir: str):
    ckpt_path = os.path.join(out_dir, "resnet50_ext.pt")
    if os.path.isfile(ckpt_path):
        # ---- í™•ì¥ ë¶„ë¥˜ê¸° ë¡œë“œ ----
        ckpt = torch.load(ckpt_path, map_location=DEVICE)
        ext_names = ckpt["ext_classnames"]  # ê¸¸ì´ 1000+Î±
        model = models.resnet50(weights=None)
        model.fc = nn.Linear(2048, len(ext_names))
        model.load_state_dict(ckpt["state_dict"])
        model = model.to(DEVICE).eval()
        tfm = tfms_eval(224)
        print(f"[INFO] Loaded extended ResNet50 checkpoint: {ckpt_path} (classes={len(ext_names)})")
        return model, ext_names, tfm, True
    else:
        # ---- í´ë°±: ImageNet 1000 í´ë˜ìŠ¤ ì‚¬ì „í•™ìŠµ ----
        w = models.ResNet50_Weights.IMAGENET1K_V1
        model = models.resnet50(weights=w).to(DEVICE).eval()
        names = w.meta["categories"]  # 1000ê°œ
        tfm = w.transforms()
        print("[WARN] Extended checkpoint not found. Falling back to ImageNet-1K pretrained ResNet50.")
        return model, names, tfm, False

@torch.no_grad()
def predict_objects_over_threshold(image_pil: Image.Image, model, classnames: List[str], tfm, thr=OBJECT_THRESHOLD):
    x = tfm(image_pil).unsqueeze(0).to(DEVICE)
    probs = torch.softmax(model(x), dim=1)[0].detach().cpu().numpy()
    idxs = np.where(probs >= thr)[0]
    if idxs.size == 0:
        return []  # ì„ê³„ì¹˜ ì´ìƒ ì—†ìŒ
    idxs = idxs[np.argsort(probs[idxs])[::-1]]  # ë‚´ë¦¼ì°¨ìˆœ
    out = []
    for i in idxs:
        # ImageNet ë¼ë²¨ì€ "tiger, Panthera tigris" í˜•íƒœê°€ ì¡´ì¬ â†’ ì²« í† í°ì„ ì‚¬ìš©
        label = classnames[i]
        tag = to_hashtag(label.split(",")[0])
        out.append((tag, float(probs[i])))
    return out

# ========== CLIP (ë¯¸ì„¸ì¡°ì •) ==========
def load_clip_finetuned_or_fallback(out_dir: str):
    ckpt_path = os.path.join(out_dir, "clip_vitb32_finetuned.pt")
    if os.path.isfile(ckpt_path):
        ckpt = torch.load(ckpt_path, map_location=DEVICE)
        model, preprocess = clip.load(ckpt.get("clip_name","ViT-B/32"), device=DEVICE)
        model.load_state_dict(ckpt["state_dict"])
        model.eval()
        print(f"[INFO] Loaded finetuned CLIP checkpoint: {ckpt_path}")
        return model, preprocess, True
    else:
        model, preprocess = clip.load("ViT-B/32", device=DEVICE)
        model.eval()
        print("[WARN] CLIP finetune checkpoint not found. Falling back to base CLIP (ViT-B/32).")
        return model, preprocess, False

@torch.no_grad()
def clip_rank(image_pil: Image.Image, clip_model, clip_preprocess, tags: List[str], template_fn, topk=2):
    img = clip_preprocess(image_pil).unsqueeze(0).to(DEVICE)
    prompts = [template_fn(t) for t in tags]
    texts = clip.tokenize(prompts).to(DEVICE)
    img_f = clip_model.encode_image(img)
    txt_f = clip_model.encode_text(texts)
    img_f = img_f / (img_f.norm(dim=-1, keepdim=True) + 1e-12)
    txt_f = txt_f / (txt_f.norm(dim=-1, keepdim=True) + 1e-12)
    sim = (img_f @ txt_f.T).softmax(dim=-1)[0].tolist()
    pairs = [(to_hashtag(tags[i]), float(sim[i])) for i in range(len(tags))]
    pairs = sorted(pairs, key=lambda x: x[1], reverse=True)[:topk]
    return pairs

# í…œí”Œë¦¿ (ê·¸ëŒ€ë¡œ ìœ ì§€)
def prompt_scene(tag): return f"a photo of a {tag} scene"
def prompt_mood(tag):  return f"a photo with a {tag} mood"

SCENE_TAGS = ["beach","sea","forest","mountain","city","indoor","outdoor","street","night","sunset","snow","desert","lake","park","skyline"]
MOOD_TAGS  = ["warm","cold","cool","retro","film","dramatic","dreamy","minimal","moody","vivid","pastel","noisy","clean","cinematic"]
# Cell 7: í†µí•© ì‹¤í–‰ + ì‹œê°í™” (ì²´í¬í¬ì¸íŠ¸ í´ë°± ê³ ë ¤)
def run_infer_and_show(image_path: str):
    if not os.path.isfile(image_path):
        raise FileNotFoundError(f"Image not found: {image_path}")

    pil = Image.open(image_path).convert("RGB")

    # 1) ResNet (í™•ì¥ or í´ë°±)
    obj_model, obj_names, obj_tfm, is_ext = load_resnet_ext_ckpt_or_fallback(OUT_OBJ_EXT)
    obj_tags = predict_objects_over_threshold(pil, obj_model, obj_names, obj_tfm, thr=OBJECT_THRESHOLD)

    # 2) CLIP (ë¯¸ì„¸ì¡°ì • or í´ë°±)
    clip_model, clip_pre, is_ft = load_clip_finetuned_or_fallback(OUT_CLIP_FT)
    scene_tags = clip_rank(pil, clip_model, clip_pre, SCENE_TAGS, prompt_scene, topk=TOPK_SCENE)
    mood_tags  = clip_rank(pil, clip_model, clip_pre, MOOD_TAGS,  prompt_mood,  topk=TOPK_MOOD)

    # 3) ì‹œê°í™”
    plt.figure(figsize=(6,6))
    plt.imshow(pil); plt.axis("off")
    title = "Uploaded Image"
    # if is_ext is False:
    #     title += "  [Object: ImageNet-1K fallback]"
    # if is_ft is False:
    #     title += "  [CLIP: base fallback]"
    plt.title(title, fontsize=14, fontweight="bold")
    plt.show()

    print("\nğŸ¯ ì„ íƒëœ í•´ì‹œíƒœê·¸")
    print("="*24)
    print("Object :", " ".join([t for t,_ in obj_tags]) if obj_tags else "-")
    print("Scene  :", " ".join([t for t,_ in scene_tags]) if scene_tags else "-")
    print("Mood   :", " ".join([t for t,_ in mood_tags]) if mood_tags else "-")

# ì˜ˆì‹œ
# run_infer_and_show("./test/tiger.jpg")
# Cell X: ëª¨ë¸ ì €ì¥(Export) - í•™ìŠµ ëë‚œ í›„ ì‹¤í–‰
import os, shutil, time

# ê¸°ë³¸ ì €ì¥ ê²½ë¡œ(í•™ìŠµ ì½”ë“œê°€ ì €ì¥í•œ ìœ„ì¹˜)
OBJ_CKPT_SRC  = "./runs_ft/object_ext/resnet50_ext.pt"
CLIP_CKPT_SRC = "./runs_ft/clip_finetune/clip_vitb32_finetuned.pt"

# ë²„ì „ í´ë”(íƒ€ì„ìŠ¤íƒ¬í”„)ë¡œ ë°±ì—… ì €ì¥
VER = time.strftime("%Y%m%d_%H%M%S")
EXPORT_DIR = f"./runs_ft/export_{VER}"
os.makedirs(EXPORT_DIR, exist_ok=True)

def safe_copy(src, dst_dir):
    if os.path.isfile(src):
        shutil.copy2(src, dst_dir)
        print(f"[SAVE] {src}  ->  {dst_dir}")
    else:
        print(f"[WARN] Not found: {src} (ì•„ì§ í•™ìŠµ ì „ì´ê±°ë‚˜ ì €ì¥ë˜ì§€ ì•ŠìŒ)")

safe_copy(OBJ_CKPT_SRC,  EXPORT_DIR)
safe_copy(CLIP_CKPT_SRC, EXPORT_DIR)

print(f"\n[INFO] Export folder: {EXPORT_DIR}")
# run_infer_and_show("./test/KakaoTalk_20251028_085926283_23.jpg")

def run_infer(image_path: str):
    if not os.path.isfile(image_path):
        raise FileNotFoundError(f"Image not found: {image_path}")

    pil = Image.open(image_path).convert("RGB")

    # 1) ResNet
    obj_model, obj_names, obj_tfm, _ = load_resnet_ext_ckpt_or_fallback(OUT_OBJ_EXT)
    obj_tags = predict_objects_over_threshold(pil, obj_model, obj_names, obj_tfm, thr=OBJECT_THRESHOLD)

    # 2) CLIP
    clip_model, clip_pre, _ = load_clip_finetuned_or_fallback(OUT_CLIP_FT)
    scene_tags = clip_rank(pil, clip_model, clip_pre, SCENE_TAGS, prompt_scene, topk=TOPK_SCENE)
    mood_tags  = clip_rank(pil, clip_model, clip_pre, MOOD_TAGS,  prompt_mood,  topk=TOPK_MOOD)

    return {
        "object_tags": [t for t,_ in obj_tags],
        "scene_tags": [t for t,_ in scene_tags],
        "mood_tags":  [t for t,_ in mood_tags]
    }