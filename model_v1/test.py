# # Cell 0: ê³µí†µ
# import os, json, math, random, re
# from typing import List, Tuple, Dict

# import numpy as np
# from PIL import Image
# from tqdm import tqdm

# import torch
# import torch.nn as nn
# from torch.utils.data import DataLoader, Dataset, random_split
# from torchvision import models, transforms, datasets

# import clip

# DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
# SEED = 42
# random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)

# ROOT = "."
# DATA_ROOT = os.path.join(ROOT, "data")
# RUNS_ROOT = os.path.join(ROOT, "runs_ft")
# os.makedirs(RUNS_ROOT, exist_ok=True)

# # í•˜ì´í¼íŒŒë¼ë¯¸í„°
# BATCH_SIZE = 32
# NUM_WORKERS = 0
# VAL_RATIO = 0.15
# TEST_RATIO = 0.15

# # ResNet í™•ì¥ í•™ìŠµ
# EPOCHS_OBJ = 8
# LR_OBJ = 3e-4
# WD_OBJ = 1e-4

# # CLIP ë¯¸ì„¸ì¡°ì • (ëŒ€ë¹„í•™ìŠµ)
# EPOCHS_CLIP = 6
# LR_CLIP = 5e-6     # CLIPì€ ì‘ì€ lr ê¶Œì¥ (í•„ìš”ì‹œ 1e-5 ~ 5e-6 ì‚¬ì´ íƒìƒ‰)
# WD_CLIP = 0.0

# # ì¶”ë¡  ê·œì¹™(ì´ì „ ìš”êµ¬ëŒ€ë¡œ ìœ ì§€)
# OBJECT_THRESHOLD = 0.20    # ê°ì²´: 0.5 ì´ìƒ ëª¨ë‘
# TOPK_SCENE = 2
# TOPK_MOOD  = 2

# def to_hashtag(s: str) -> str:
#     return "#" + s.strip().replace(" ", "_")
# # Cell 1: Transform / Split / Helper
# def tfms_train(size=224):
#     return transforms.Compose([
#         transforms.Resize((size, size)),
#         transforms.RandomHorizontalFlip(),
#         transforms.ColorJitter(0.2,0.2,0.2,0.1),
#         transforms.ToTensor(),
#         transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),
#     ])

# def tfms_eval(size=224):
#     return transforms.Compose([
#         transforms.Resize((size, size)),
#         transforms.ToTensor(),
#         transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),
#     ])

# def build_splits(ds, val_ratio=VAL_RATIO, test_ratio=TEST_RATIO):
#     n = len(ds)
#     n_val = int(n * val_ratio)
#     n_test = int(n * test_ratio)
#     n_train = n - n_val - n_test
#     return random_split(ds, [n_train, n_val, n_test], generator=torch.Generator().manual_seed(SEED))

# def clean_caption_from_filename(name: str) -> str:
#     # íŒŒì¼ëª… -> ìº¡ì…˜: í™•ì¥ì ì œê±°, ì–¸ë”ìŠ¤ì½”ì–´/ëŒ€ì‹œë¥¼ ê³µë°±ìœ¼ë¡œ
#     base = os.path.splitext(os.path.basename(name))[0]
#     base = re.sub(r'[_\-]+', ' ', base)
#     return base.strip()
# # Cell 2: ResNet í™•ì¥ ë¶„ë¥˜ê¸° (ê¸°ì¡´ 1000 + ìƒˆ í´ë˜ìŠ¤)
# def load_imagenet_resnet50():
#     w = models.ResNet50_Weights.IMAGENET1K_V1
#     base = models.resnet50(weights=w)
#     base.eval()
#     imagenet_classes = w.meta["categories"]  # ê¸¸ì´ 1000
#     return base, imagenet_classes

# def build_extended_fc(old_fc: nn.Linear, num_new: int) -> nn.Linear:
#     in_dim = old_fc.in_features
#     out_old = old_fc.out_features
#     out_new = out_old + num_new
#     new_fc = nn.Linear(in_dim, out_new)
#     # ê¸°ì¡´ ê°€ì¤‘ì¹˜/ë°”ì´ì–´ìŠ¤ ë³µì‚¬
#     with torch.no_grad():
#         new_fc.weight[:out_old] = old_fc.weight
#         new_fc.bias[:out_old]   = old_fc.bias
#         # ë‚˜ë¨¸ì§€ ìƒˆ í–‰ì€ Kaiming/Zero ì´ˆê¸°í™”
#         nn.init.kaiming_normal_(new_fc.weight[out_old:])
#         nn.init.constant_(new_fc.bias[out_old:], 0.0)
#     return new_fc

# def map_ext_classes(object_root: str, imagenet_classes: List[str]):
#     """
#     object_ext í´ë”ì˜ í´ë˜ìŠ¤ëª…ì„ í™•ì¸:
#     - í´ë”ëª…ì´ ImageNet í´ë˜ìŠ¤ëª…ê³¼ 'ì™„ì „ ì¼ì¹˜'í•˜ë©´ old í´ë˜ìŠ¤ë¡œ ê°„ì£¼(ê°•í™”í•™ìŠµ ëŒ€ìƒ)
#     - ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ new í´ë˜ìŠ¤ë¡œ ê°„ì£¼(í™•ì¥)
#     """
#     # ImageFolderë¡œ í´ë˜ìŠ¤ëª… í™•ë³´
#     ds_tmp = datasets.ImageFolder(object_root, transform=tfms_train())
#     custom_classes = ds_tmp.classes  # í´ë”ëª… ì‚¬ì „ìˆœ
#     # ë§¤í•‘
#     name_to_idx_imagenet = {name: i for i, name in enumerate(imagenet_classes)}
#     old_indices, new_names = [], []
#     for cname in custom_classes:
#         if cname in name_to_idx_imagenet:
#             old_indices.append(name_to_idx_imagenet[cname])
#         else:
#             new_names.append(cname)
#     return old_indices, new_names, custom_classes

# def make_extended_loaders(object_root: str, size=224):
#     return build_splits(datasets.ImageFolder(object_root, transform=tfms_train(size)))
# # Cell 3: ResNet í™•ì¥ í•™ìŠµ
# def train_resnet_extended(object_root: str, out_dir: str, epochs=EPOCHS_OBJ, lr=LR_OBJ, wd=WD_OBJ):
#     os.makedirs(out_dir, exist_ok=True)
#     base, imagenet_classes = load_imagenet_resnet50()
#     base = base.to(DEVICE).train()

#     # ì–´ë–¤ í´ë˜ìŠ¤ê°€ old(ê¸°ì¡´ 1000)ì´ê³ , ì–´ë–¤ ê²Œ newì¸ì§€ íŒë³„
#     old_indices, new_names, custom_classes = map_ext_classes(object_root, imagenet_classes)
#     num_new = len(new_names)
#     print(f"Existing(imagenet) matched: {len(old_indices)} | New classes to add: {num_new}")

#     # í™•ì¥ FC êµ¬ì„±
#     base.fc = build_extended_fc(base.fc, num_new).to(DEVICE)

#     # í™•ì¥ëœ í´ë˜ìŠ¤ ì´ë¦„ í…Œì´ë¸”(ê¸¸ì´ 1000+num_new)
#     ext_classnames = list(imagenet_classes) + new_names

#     # ìš°ë¦¬ ë°ì´í„°ì…‹ ë¡œë”
#     train_ds, val_ds, test_ds = make_extended_loaders(object_root)
#     # eval transform ì ìš©
#     val_ds.dataset.transform = tfms_eval()
#     test_ds.dataset.transform = tfms_eval()
#     tr = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS)
#     va = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)
#     te = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)

#     # ë¼ë²¨ ë§¤í•‘: ì»¤ìŠ¤í…€ í´ë˜ìŠ¤ â†’ í™•ì¥ ì¸ë±ìŠ¤
#     # - í´ë”ëª…ì´ ImageNetì— ìˆìœ¼ë©´ â†’ ê·¸ index ì‚¬ìš©
#     # - ì—†ìœ¼ë©´ â†’ 1000 + ìƒˆ í´ë˜ìŠ¤ì˜ index (custom_classes ìˆœì„œ ê¸°ë°˜ ë§¤í•‘ í•„ìš”)
#     name_to_idx_imagenet = {n:i for i,n in enumerate(imagenet_classes)}
#     new_name_to_ext = {n: 1000 + i for i,n in enumerate(new_names)}
#     # ImageFolderì˜ class_to_idxë¡œ ë°°ì¹˜ì˜ yë¥¼ ext ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” ë˜í¼
#     def to_ext_idx(y: torch.Tensor):
#         # yëŠ” ImageFolder ê¸°ì¤€ ì¸ë±ìŠ¤ â†’ í•´ë‹¹ ì´ë¦„ì„ ì°¾ì•„ extë¡œ ë³€í™˜
#         inv = {v:k for k,v in train_ds.dataset.class_to_idx.items()}
#         out = y.clone()
#         for j in range(y.shape[0]):
#             cname = inv[int(y[j].item())]
#             if cname in name_to_idx_imagenet:
#                 out[j] = name_to_idx_imagenet[cname]
#             else:
#                 out[j] = new_name_to_ext[cname]
#         return out.to(DEVICE)

#     # ì˜µí‹°ë§ˆì´ì € (ì „ì¸µ ë¯¸ì„¸ì¡°ì •)
#     opt = torch.optim.AdamW(base.parameters(), lr=lr, weight_decay=wd)
#     criterion = nn.CrossEntropyLoss()

#     def epoch_loop(loader, train_flag=True):
#         base.train() if train_flag else base.eval()
#         tot, corr, n = 0.0, 0, 0
#         for x, y in loader:
#             x = x.to(DEVICE)
#             y = to_ext_idx(y)  # â† í™•ì¥ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
#             if train_flag: opt.zero_grad()
#             logits = base(x)
#             loss = criterion(logits, y)
#             if train_flag:
#                 loss.backward(); opt.step()
#             tot += loss.item()*x.size(0)
#             pred = logits.argmax(1)
#             corr += (pred==y).sum().item()
#             n += x.size(0)
#         return tot/n, corr/n

#     best_acc, ckpt_path = -1, os.path.join(out_dir, "resnet50_ext.pt")
#     for ep in range(1, epochs+1):
#         tl, ta = epoch_loop(tr, True)
#         vl, va_acc = epoch_loop(va, False)
#         print(f"[EXT][{ep}/{epochs}] train {tl:.4f}/{ta:.3f} | val {vl:.4f}/{va_acc:.3f}")
#         if va_acc > best_acc:
#             best_acc = va_acc
#             torch.save({
#                 "state_dict": base.state_dict(),
#                 "ext_classnames": ext_classnames,  # ê¸¸ì´ 1000+Î±
#             }, ckpt_path)

#     # test
#     tl, ta = epoch_loop(te, False)
#     print(f"[EXT][TEST] loss {tl:.4f} acc {ta:.3f}")
#     return ckpt_path, ext_classnames
# # Cell 4: CLIP ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ëŒ€ë¹„í•™ìŠµ ë¯¸ì„¸ì¡°ì •
# class CaptionImageDataset(Dataset):
#     def __init__(self, folder: str, preprocess, tokenizer):
#         self.paths = []
#         for f in os.listdir(folder):
#             p = os.path.join(folder, f)
#             if os.path.isfile(p) and os.path.splitext(p)[1].lower() in (".jpg",".jpeg",".png",".webp",".bmp"):
#                 self.paths.append(p)
#         self.paths.sort()
#         self.preprocess = preprocess
#         self.tokenizer = tokenizer

#     def __len__(self): return len(self.paths)
#     def __getitem__(self, idx):
#         p = self.paths[idx]
#         img = Image.open(p).convert("RGB")
#         x = self.preprocess(img)
#         caption = clean_caption_from_filename(p)
#         return x, caption

# def clip_contrastive_train(folder: str, out_dir: str, epochs=EPOCHS_CLIP, lr=LR_CLIP, wd=WD_CLIP):
#     os.makedirs(out_dir, exist_ok=True)
#     model, preprocess = clip.load("ViT-B/32", device=DEVICE)
#     ds_full = CaptionImageDataset(folder, preprocess, clip.tokenize)
#     # train/val split
#     n = len(ds_full)
#     n_val = max(1, int(n*VAL_RATIO))
#     n_train = n - n_val
#     tr_ds, va_ds = random_split(ds_full, [n_train, n_val], generator=torch.Generator().manual_seed(SEED))
#     tr = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS)
#     va = DataLoader(va_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)

#     # ì˜µí‹°ë§ˆì´ì €: ì „ì²´ íŒŒë¼ë¯¸í„°(ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ì¸ì½”ë” í¬í•¨) ë¯¸ì„¸ì¡°ì • (ì›í•˜ë©´ ì¼ë¶€ë§Œ í’€ ìˆ˜ë„ ìˆìŒ)
#     opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)

#     def run_epoch(loader, train_flag=True):
#         model.train() if train_flag else model.eval()
#         tot, n = 0.0, 0
#         for imgs, caps in loader:
#             imgs = imgs.to(DEVICE)
#             tokens = clip.tokenize(caps, truncate=True).to(DEVICE)
#             if train_flag: opt.zero_grad()
#             logits_per_image, logits_per_text = model(imgs, tokens)
#             # CLIPì˜ ê¸°ë³¸ ëŒ€ë¹„í•™ìŠµ ì†ì‹¤ = ì´ë¯¸ì§€â†’í…ìŠ¤íŠ¸, í…ìŠ¤íŠ¸â†’ì´ë¯¸ì§€ CE í‰ê· 
#             labels = torch.arange(len(imgs), device=DEVICE)
#             loss_i = nn.CrossEntropyLoss()(logits_per_image, labels)
#             loss_t = nn.CrossEntropyLoss()(logits_per_text, labels)
#             loss = (loss_i + loss_t) / 2
#             if train_flag:
#                 loss.backward(); opt.step()
#             tot += loss.item()*imgs.size(0); n += imgs.size(0)
#         return tot/max(1,n)

#     best_val, ckpt_path = 1e9, os.path.join(out_dir, "clip_vitb32_finetuned.pt")
#     for ep in range(1, epochs+1):
#         tl = run_epoch(tr, True)
#         vl = run_epoch(va, False)
#         print(f"[CLIP-FT][{ep}/{epochs}] train loss {tl:.4f} | val loss {vl:.4f}")
#         if vl < best_val:
#             best_val = vl
#             torch.save({"clip_name":"ViT-B/32","state_dict":model.state_dict()}, ckpt_path)

#     return ckpt_path
# # Cell 5: í•™ìŠµ ì‹¤í–‰ ìŠ¤ìœ„ì¹˜
# OBJ_EXT_ROOT = os.path.join(DATA_ROOT, "object_ext")
# CLIP_PAIRS   = os.path.join(DATA_ROOT, "clip_pairs")

# OUT_OBJ_EXT  = os.path.join(RUNS_ROOT, "object_ext")
# OUT_CLIP_FT  = os.path.join(RUNS_ROOT, "clip_finetune")

# DO_TRAIN_RESNET_EXT = False
# DO_TRAIN_CLIP_FT    = False

# if DO_TRAIN_RESNET_EXT:
#     path_obj_ckpt, ext_names = train_resnet_extended(OBJ_EXT_ROOT, OUT_OBJ_EXT)

# if DO_TRAIN_CLIP_FT:
#     path_clip_ckpt = clip_contrastive_train(CLIP_PAIRS, OUT_CLIP_FT)
# # Cell 6: ì¶”ë¡  ìœ í‹¸ (ì²´í¬í¬ì¸íŠ¸ ì—†ìœ¼ë©´ ìë™ í´ë°±)
# import matplotlib.pyplot as plt

# # ========== ResNet (í™•ì¥) ==========
# def load_resnet_ext_ckpt_or_fallback(out_dir: str):
#     ckpt_path = os.path.join(out_dir, "resnet50_ext.pt")
#     if os.path.isfile(ckpt_path):
#         # ---- í™•ì¥ ë¶„ë¥˜ê¸° ë¡œë“œ ----
#         ckpt = torch.load(ckpt_path, map_location=DEVICE)
#         ext_names = ckpt["ext_classnames"]  # ê¸¸ì´ 1000+Î±
#         model = models.resnet50(weights=None)
#         model.fc = nn.Linear(2048, len(ext_names))
#         model.load_state_dict(ckpt["state_dict"])
#         model = model.to(DEVICE).eval()
#         tfm = tfms_eval(224)
#         print(f"[INFO] Loaded extended ResNet50 checkpoint: {ckpt_path} (classes={len(ext_names)})")
#         return model, ext_names, tfm, True
#     else:
#         # ---- í´ë°±: ImageNet 1000 í´ë˜ìŠ¤ ì‚¬ì „í•™ìŠµ ----
#         w = models.ResNet50_Weights.IMAGENET1K_V1
#         model = models.resnet50(weights=w).to(DEVICE).eval()
#         names = w.meta["categories"]  # 1000ê°œ
#         tfm = w.transforms()
#         print("[WARN] Extended checkpoint not found. Falling back to ImageNet-1K pretrained ResNet50.")
#         return model, names, tfm, False

# @torch.no_grad()
# def predict_objects_over_threshold(image_pil: Image.Image, model, classnames: List[str], tfm, thr=OBJECT_THRESHOLD):
#     x = tfm(image_pil).unsqueeze(0).to(DEVICE)
#     probs = torch.softmax(model(x), dim=1)[0].detach().cpu().numpy()
#     idxs = np.where(probs >= thr)[0]
#     if idxs.size == 0:
#         return []  # ì„ê³„ì¹˜ ì´ìƒ ì—†ìŒ
#     idxs = idxs[np.argsort(probs[idxs])[::-1]]  # ë‚´ë¦¼ì°¨ìˆœ
#     out = []
#     for i in idxs:
#         # ImageNet ë¼ë²¨ì€ "tiger, Panthera tigris" í˜•íƒœê°€ ì¡´ì¬ â†’ ì²« í† í°ì„ ì‚¬ìš©
#         label = classnames[i]
#         tag = to_hashtag(label.split(",")[0])
#         out.append((tag, float(probs[i])))
#     return out

# # ========== CLIP (ë¯¸ì„¸ì¡°ì •) ==========
# def load_clip_finetuned_or_fallback(out_dir: str):
#     ckpt_path = os.path.join(out_dir, "clip_vitb32_finetuned.pt")
#     if os.path.isfile(ckpt_path):
#         ckpt = torch.load(ckpt_path, map_location=DEVICE)
#         model, preprocess = clip.load(ckpt.get("clip_name","ViT-B/32"), device=DEVICE)
#         model.load_state_dict(ckpt["state_dict"])
#         model.eval()
#         print(f"[INFO] Loaded finetuned CLIP checkpoint: {ckpt_path}")
#         return model, preprocess, True
#     else:
#         model, preprocess = clip.load("ViT-B/32", device=DEVICE)
#         model.eval()
#         print("[WARN] CLIP finetune checkpoint not found. Falling back to base CLIP (ViT-B/32).")
#         return model, preprocess, False

# @torch.no_grad()
# def clip_rank(image_pil: Image.Image, clip_model, clip_preprocess, tags: List[str], template_fn, topk=2):
#     img = clip_preprocess(image_pil).unsqueeze(0).to(DEVICE)
#     prompts = [template_fn(t) for t in tags]
#     texts = clip.tokenize(prompts).to(DEVICE)
#     img_f = clip_model.encode_image(img)
#     txt_f = clip_model.encode_text(texts)
#     img_f = img_f / (img_f.norm(dim=-1, keepdim=True) + 1e-12)
#     txt_f = txt_f / (txt_f.norm(dim=-1, keepdim=True) + 1e-12)
#     sim = (img_f @ txt_f.T).softmax(dim=-1)[0].tolist()
#     pairs = [(to_hashtag(tags[i]), float(sim[i])) for i in range(len(tags))]
#     pairs = sorted(pairs, key=lambda x: x[1], reverse=True)[:topk]
#     return pairs

# # í…œí”Œë¦¿ (ê·¸ëŒ€ë¡œ ìœ ì§€)
# def prompt_scene(tag): return f"a photo of a {tag} scene"
# def prompt_mood(tag):  return f"a photo with a {tag} mood"

# SCENE_TAGS = ["beach","sea","forest","mountain","city","indoor","outdoor","street","night","sunset","snow","desert","lake","park","skyline"]
# MOOD_TAGS  = ["warm","cold","cool","retro","film","dramatic","dreamy","minimal","moody","vivid","pastel","noisy","clean","cinematic"]
# # Cell 7: í†µí•© ì‹¤í–‰ + ì‹œê°í™” (ì²´í¬í¬ì¸íŠ¸ í´ë°± ê³ ë ¤)
# def run_infer_and_show(image_path: str):
#     if not os.path.isfile(image_path):
#         raise FileNotFoundError(f"Image not found: {image_path}")

#     pil = Image.open(image_path).convert("RGB")

#     # 1) ResNet (í™•ì¥ or í´ë°±)
#     obj_model, obj_names, obj_tfm, is_ext = load_resnet_ext_ckpt_or_fallback(OUT_OBJ_EXT)
#     obj_tags = predict_objects_over_threshold(pil, obj_model, obj_names, obj_tfm, thr=OBJECT_THRESHOLD)

#     # 2) CLIP (ë¯¸ì„¸ì¡°ì • or í´ë°±)
#     clip_model, clip_pre, is_ft = load_clip_finetuned_or_fallback(OUT_CLIP_FT)
#     scene_tags = clip_rank(pil, clip_model, clip_pre, SCENE_TAGS, prompt_scene, topk=TOPK_SCENE)
#     mood_tags  = clip_rank(pil, clip_model, clip_pre, MOOD_TAGS,  prompt_mood,  topk=TOPK_MOOD)

#     # 3) ì‹œê°í™”
#     plt.figure(figsize=(6,6))
#     plt.imshow(pil); plt.axis("off")
#     title = "Uploaded Image"
#     # if is_ext is False:
#     #     title += "  [Object: ImageNet-1K fallback]"
#     # if is_ft is False:
#     #     title += "  [CLIP: base fallback]"
#     plt.title(title, fontsize=14, fontweight="bold")
#     plt.show()

#     print("\nğŸ¯ ì„ íƒëœ í•´ì‹œíƒœê·¸")
#     print("="*24)
#     print("Object :", " ".join([t for t,_ in obj_tags]) if obj_tags else "-")
#     print("Scene  :", " ".join([t for t,_ in scene_tags]) if scene_tags else "-")
#     print("Mood   :", " ".join([t for t,_ in mood_tags]) if mood_tags else "-")

# # ì˜ˆì‹œ
# # run_infer_and_show("./test/tiger.jpg")
# # Cell X: ëª¨ë¸ ì €ì¥(Export) - í•™ìŠµ ëë‚œ í›„ ì‹¤í–‰
# import os, shutil, time

# # ê¸°ë³¸ ì €ì¥ ê²½ë¡œ(í•™ìŠµ ì½”ë“œê°€ ì €ì¥í•œ ìœ„ì¹˜)
# OBJ_CKPT_SRC  = "./runs_ft/object_ext/resnet50_ext.pt"
# CLIP_CKPT_SRC = "./runs_ft/clip_finetune/clip_vitb32_finetuned.pt"

# # ë²„ì „ í´ë”(íƒ€ì„ìŠ¤íƒ¬í”„)ë¡œ ë°±ì—… ì €ì¥
# VER = time.strftime("%Y%m%d_%H%M%S")
# EXPORT_DIR = f"./runs_ft/export_{VER}"
# os.makedirs(EXPORT_DIR, exist_ok=True)

# def safe_copy(src, dst_dir):
#     if os.path.isfile(src):
#         shutil.copy2(src, dst_dir)
#         print(f"[SAVE] {src}  ->  {dst_dir}")
#     else:
#         print(f"[WARN] Not found: {src} (ì•„ì§ í•™ìŠµ ì „ì´ê±°ë‚˜ ì €ì¥ë˜ì§€ ì•ŠìŒ)")

# safe_copy(OBJ_CKPT_SRC,  EXPORT_DIR)
# safe_copy(CLIP_CKPT_SRC, EXPORT_DIR)

# print(f"\n[INFO] Export folder: {EXPORT_DIR}")
# # run_infer_and_show("./test/KakaoTalk_20251028_085926283_23.jpg")

# def run_infer(image_path: str):
#     if not os.path.isfile(image_path):
#         raise FileNotFoundError(f"Image not found: {image_path}")

#     pil = Image.open(image_path).convert("RGB")

#     # 1) ResNet
#     obj_model, obj_names, obj_tfm, _ = load_resnet_ext_ckpt_or_fallback(OUT_OBJ_EXT)
#     obj_tags = predict_objects_over_threshold(pil, obj_model, obj_names, obj_tfm, thr=OBJECT_THRESHOLD)

#     # 2) CLIP
#     clip_model, clip_pre, _ = load_clip_finetuned_or_fallback(OUT_CLIP_FT)
#     scene_tags = clip_rank(pil, clip_model, clip_pre, SCENE_TAGS, prompt_scene, topk=TOPK_SCENE)
#     mood_tags  = clip_rank(pil, clip_model, clip_pre, MOOD_TAGS,  prompt_mood,  topk=TOPK_MOOD)

#     return {
#         "object_tags": [t for t,_ in obj_tags],
#         "scene_tags": [t for t,_ in scene_tags],
#         "mood_tags":  [t for t,_ in mood_tags]
#     }


# ===============================
# Cell 1. í™˜ê²½/ì„í¬íŠ¸
# ===============================
import os, random
from pathlib import Path
from typing import List, Tuple, Dict

import torch
import torchvision.transforms as T
import torchvision.models as models
from PIL import Image
import matplotlib.pyplot as plt

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
SEED = 42
random.seed(SEED)
torch.manual_seed(SEED)

print(f"[INFO] DEVICE = {DEVICE}")

# ===============================
# Cell 2. ê³µí†µ ë¼ë²¨/ì¹´í…Œê³ ë¦¬/í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸
# ===============================

# (1) ë¬´ë“œ ë¼ë²¨ í™•ì¥
MOOD_LABELS = [
    # ì˜¨ë„/ìƒ‰ê°
    "warm","cool","neutral","soft","bright","dark","colorful","monochrome",
    # ê°ì •
    "romantic","happy","joyful","peaceful","melancholic","lonely",
    "dreamy","hopeful","mysterious","dramatic","nostalgic","energetic",
    "relaxed","cozy","moody","serene","intense","calm","fresh",
    # ìŠ¤íƒ€ì¼
    "retro","vintage","film look","cinematic","minimal","luxury","elegant",
    "grunge","aesthetic","modern","futuristic","classic","vivid","dynamic",
    "bold","natural","organic","urban","clean","artistic","pastel",
    # ì‹œê°„/ê³„ì ˆ
    "spring","summer","autumn","winter","sunset","night","morning",
]

# (2) ì¹´í…Œê³ ë¦¬ 7ê°œ â†’ ë‹¤ì¤‘ í”„ë¡¬í”„íŠ¸
CATEGORY_PROMPTS = {
    "ì‚¬ëŒ": [
        "a portrait photo of a person or people",
        "group of friends interacting outdoors",
        "family at home or in a park",
        "close-up of human face showing emotion",
        "lifestyle scene with people working or studying",
        "street photography with people in frame",
    ],
    "ì˜ˆìˆ ": [
        "an artistic and creative visual composition",
        "abstract colorful illustration artwork",
        "digital painting or conceptual art",
        "fashion or editorial artistic photography",
        "minimalist geometric artwork",
    ],
    "í’ê²½": [
        "a natural landscape scenery with mountains or rivers",
        "outdoor nature scene with sky, forest or field",
        "sunset or sunrise over nature",
        "travel destination with scenic view",
        "aerial drone view of nature or sea",
    ],
    "ë™ë¬¼": [
        "wildlife animal photo in natural habitat",
        "close-up portrait of an animal",
        "domestic pet like cat or dog",
        "bird or marine animal in motion",
    ],
    "ìŒì‹": [
        "delicious food and drink close-up photo",
        "tabletop with meals, coffee, or desserts",
        "chef cooking or restaurant food plating",
        "traditional cultural dish or street food",
    ],
    "ì‚°ì—…": [
        "factory or manufacturing environment with machines",
        "engineering or technology workplace",
        "business or scientific industrial scene",
        "laboratory or research equipment",
        "construction or production line",
    ],
    "ê±´ì¶•": [
        "modern architectural building exterior",
        "interior design of home or office",
        "urban cityscape with buildings and lights",
        "historic or cultural architecture",
        "minimalist architecture photography",
    ],
}

# (3) ê°ì²´ í•„í„°ë§ìš© í™”ì´íŠ¸/ë¸”ë™ë¦¬ìŠ¤íŠ¸
#    â†’ ImageNet 1000ê°œ ì¤‘ ë„ˆë¬´ ì• ë§¤í•œ ê±´ ê±¸ëŸ¬ì£¼ê¸° ìœ„í•¨
ALLOWED_OBJECTS = {
    # ë™ë¬¼
    "tiger","lion","cat","kitten","dog","puppy","fox","wolf","leopard","zebra","horse","cow","sheep","goat","elephant",
    # ìŒì‹
    "pizza","hamburger","cheeseburger","hotdog","espresso","coffee","cup","teapot","plate","ice cream","sushi",
    # ì‚¬ë¬¼/ì œí’ˆ
    "car","sports car","minivan","pickup","bus","bicycle","motor scooter","airliner","boat",
    "cellular telephone","laptop","notebook","desktop computer","keyboard","monitor","television",
    "guitar","violin","drum","microphone","camera",
    # ìì—°ë¬¼
    "tree","pine tree","palm tree","coral reef","lake","cliff",
    # ì‚¬ëŒ ë¹„ìŠ·í•œ í´ë˜ìŠ¤
    "person","scuba diver","bridegroom","groom","suit",
}

BLOCK_OBJECTS = {
    "web_site","menu","screen","projector","computer keyboard","monitor","television","pickup_coil"
}
# ===============================
# Cell 3. CLIP ì „ì—­ ë¡œë“œ (1íšŒ)
# ===============================
def load_clip(model_name="ViT-B-32", pretrained="laion2b_s34b_b79k"):
    import open_clip
    model, _, preprocess = open_clip.create_model_and_transforms(
        model_name, pretrained=pretrained, device=DEVICE
    )
    tok = open_clip.get_tokenizer(model_name)
    model.eval()
    return model, preprocess, tok

CLIP_MODEL, CLIP_PREP, CLIP_TOK = load_clip()

def clip_encode_image(pil: Image.Image) -> torch.Tensor:
    with torch.no_grad():
        img = CLIP_PREP(pil).unsqueeze(0).to(DEVICE)
        return CLIP_MODEL.encode_image(img)

def clip_encode_text(prompts: List[str]) -> torch.Tensor:
    with torch.no_grad():
        txt = CLIP_TOK(prompts).to(DEVICE)
        return CLIP_MODEL.encode_text(txt)

def cosine_sim(a: torch.Tensor, b: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:
    a = a / (a.norm(dim=-1, keepdim=True) + eps)
    b = b / (b.norm(dim=-1, keepdim=True) + eps)
    return a @ b.T

def softmax_normalize(x: torch.Tensor) -> torch.Tensor:
    x = x - x.max()
    ex = torch.exp(x)
    return ex / ex.sum()
# ===============================
# Cell 4. ResNet-50 (ImageNet) â€“ ê°ì²´
# ===============================
import requests

IMAGENET_URL = "https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
IMAGENET_CLASSES = requests.get(IMAGENET_URL).text.splitlines()

resnet_obj = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2).to(DEVICE)
resnet_obj.eval()

transform_imagenet = T.Compose([
    T.Resize(256),
    T.CenterCrop(224),
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225])
])

def infer_resnet_objects_raw(pil_img: Image.Image, topk: int = 5) -> List[str]:
    img_t = transform_imagenet(pil_img).unsqueeze(0).to(DEVICE)
    with torch.no_grad():
        out = resnet_obj(img_t)
        probs = torch.nn.functional.softmax(out, dim=1)
        vals, idxs = torch.topk(probs, topk)
    return [IMAGENET_CLASSES[i] for i in idxs[0]]

def postprocess_objects(preds: List[str], want: int = 3) -> List[str]:
    # 1) ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì œê±° â†’ 2) í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ìš°ì„  â†’ 3) ë‚¨ëŠ”ê±° ì±„ìš°ê¸°
    cleaned = []
    for p in preds:
        norm_p = p.replace("_"," ")
        if norm_p in BLOCK_OBJECTS:
            continue
        if (ALLOWED_OBJECTS and norm_p in ALLOWED_OBJECTS) or not ALLOWED_OBJECTS:
            cleaned.append(norm_p)
    # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ë¡œ 3ê°œê°€ ì•ˆ ì±„ì›Œì¡Œìœ¼ë©´ ì›ë˜ ì˜ˆì¸¡ì—ì„œ ì±„ì›€
    if len(cleaned) < want:
        for p in preds:
            norm_p = p.replace("_"," ")
            if norm_p not in cleaned and norm_p not in BLOCK_OBJECTS:
                cleaned.append(norm_p)
            if len(cleaned) >= want:
                break
    return [f"#{c.replace(' ','_')}" for c in cleaned[:want]]

def infer_objects(pil_img: Image.Image, topk: int = 3) -> List[str]:
    raw = infer_resnet_objects_raw(pil_img, topk=5)
    return postprocess_objects(raw, want=topk)
# ===============================
# Cell 5. ResNet-Places â€“ ì¥ì†Œ
# ===============================
import urllib

PLACES_WEIGHTS = "resnet50_places365.pth.tar"
PLACES_LABELS  = "categories_places365.txt"

if not Path(PLACES_WEIGHTS).exists():
    urllib.request.urlretrieve(
        "http://places2.csail.mit.edu/models_places365/resnet50_places365.pth.tar",
        PLACES_WEIGHTS
    )

if not Path(PLACES_LABELS).exists():
    urllib.request.urlretrieve(
        "https://raw.githubusercontent.com/csailvision/places365/master/categories_places365.txt",
        PLACES_LABELS
    )

classes_places = [line.strip().split(" ")[0][3:] for line in open(PLACES_LABELS)]

resnet_places = models.resnet50(num_classes=365)
checkpoint = torch.load(PLACES_WEIGHTS, map_location=DEVICE)
state_dict = {k.replace("module.",""): v for k,v in checkpoint["state_dict"].items()}
resnet_places.load_state_dict(state_dict)
resnet_places.eval().to(DEVICE)

transform_places = T.Compose([
    T.Resize((256,256)),
    T.CenterCrop(224),
    T.ToTensor(),
    T.Normalize(mean=[0.485,0.456,0.406],
                std=[0.229,0.224,0.225])
])

def infer_scene(pil_img: Image.Image, topk: int = 3) -> List[str]:
    img_t = transform_places(pil_img).unsqueeze(0).to(DEVICE)
    with torch.no_grad():
        logit = resnet_places(img_t)
        probs = torch.nn.functional.softmax(logit, dim=1)
        vals, idxs = torch.topk(probs, topk)
    labels = [classes_places[i] for i in idxs[0]]
    return [f"#{l.replace(' ','_')}" for l in labels]
# ===============================
# Cell 6. CLIP â€“ ë¬´ë“œ + ì¹´í…Œê³ ë¦¬ (ë‹¤ì¤‘ í”„ë¡¬í”„íŠ¸ í‰ê· )
# ===============================
def infer_mood_clip(pil_img: Image.Image, topk: int = 3) -> List[str]:
    img_feat = clip_encode_image(pil_img)

    mood_scores = []
    for mood in MOOD_LABELS:
        prompts = [
            f"an image with a {mood} atmosphere",
            f"a photo that feels {mood}",
            f"a scene with {mood} mood",
        ]
        txt_feat = clip_encode_text(prompts)
        sims = cosine_sim(img_feat, txt_feat)  # 1 x N
        score = sims.mean().item()
        mood_scores.append((mood, score))

    # ì ìˆ˜ ë†’ì€ ìˆœ
    mood_scores.sort(key=lambda x: x[1], reverse=True)
    return [f"#{m[0].replace(' ','_')}" for m in mood_scores[:topk]]

def infer_categories_clip(pil_img: Image.Image) -> Dict[str, float]:
    img_feat = clip_encode_image(pil_img)
    cat_scores = {}
    for cat, prompts in CATEGORY_PROMPTS.items():
        txt_feat = clip_encode_text(prompts)
        sims = cosine_sim(img_feat, txt_feat)  # 1 x N
        score = sims.mean().item()
        cat_scores[cat] = score
    # softmaxë¡œ í™•ë¥ í™”
    scores_tensor = torch.tensor(list(cat_scores.values())).unsqueeze(0)
    probs = softmax_normalize(scores_tensor).flatten().tolist()
    final = {}
    for i, cat in enumerate(cat_scores.keys()):
        final[cat] = probs[i]
    return final  # {"ì‚¬ëŒ":0.12, ...}

# ===============================
# Cell 7. íƒœê·¸ íˆ¬í‘œ + CLIP ì ìˆ˜ í˜¼í•©
# ===============================
def category_votes_from_tags(obj_tags: List[str], scene_tags: List[str]) -> Dict[str, float]:
    votes = {c: 0.0 for c in CATEGORY_PROMPTS.keys()}

    # ë™ë¬¼
    animal_keywords = ["tiger","lion","cat","dog","horse","elephant","bear","zebra"]
    if any(any(k in t for k in animal_keywords) for t in obj_tags):
        votes["ë™ë¬¼"] += 0.6

    # í’ê²½
    nature_keywords = ["forest","park","beach","mountain","lake","desert","waterfall","field"]
    if any(any(k in t for k in nature_keywords) for t in scene_tags):
        votes["í’ê²½"] += 0.5

    # ê±´ì¶•
    arch_keywords = ["office","factory","library","temple","church","stadium","hotel","airport","street","city"]
    if any(any(k in t for k in scene_tags) for t in arch_keywords):
        votes["ê±´ì¶•"] += 0.4

    # ì‚°ì—…
    ind_keywords = ["factory","industrial","lab","laboratory"]
    if any(any(k in t for k in scene_tags) for t in ind_keywords):
        votes["ì‚°ì—…"] += 0.4

    # ìŒì‹
    food_keywords = ["restaurant","kitchen","cafe","dining"]
    if any(any(k in t for k in scene_tags) for t in food_keywords):
        votes["ìŒì‹"] += 0.35

    return votes

def mix_category_scores(clip_scores: Dict[str, float],
                        votes: Dict[str, float],
                        w_clip: float = 0.7,
                        w_vote: float = 0.3) -> List[Tuple[str, float]]:
    mixed = {}
    for cat in clip_scores.keys():
        mixed[cat] = clip_scores[cat] * w_clip + votes.get(cat, 0.0) * w_vote
    # normalize
    s = sum(mixed.values()) or 1.0
    mixed = {k: v/s for k,v in mixed.items()}
    # sort
    mixed_sorted = sorted(mixed.items(), key=lambda x: x[1], reverse=True)
    return mixed_sorted
# ===============================
# Cell 8. í†µí•© ì‹¤í–‰ í•¨ìˆ˜
# ===============================
def run_full_infer(pil: Image.Image):
    # 1) ê°ì²´ 3
    obj_tags = infer_objects(pil, topk=3)

    # 2) ì¥ì†Œ 3
    scene_tags = infer_scene(pil, topk=3)

    # 3) ë¶„ìœ„ê¸° 3 (CLIP)
    mood_tags = infer_mood_clip(pil, topk=3)

    # 4) ì¹´í…Œê³ ë¦¬: CLIP + íƒœê·¸íˆ¬í‘œ í˜¼í•©
    clip_cat_scores = infer_categories_clip(pil)
    votes = category_votes_from_tags(obj_tags, scene_tags)
    mixed = mix_category_scores(clip_cat_scores, votes, w_clip=0.7, w_vote=0.3)
    top3_cat = mixed[:3]

    return {
        "object_tags": obj_tags,
        "scene_tags": scene_tags,
        "mood_tags": mood_tags,
        "categories": top3_cat,
    }

# ===============================
# Cell 9. ì‹¤ì œ ì‹¤í–‰ & ì¶œë ¥
# ===============================
# TEST_IMG = "./test//KakaoTalk_20251028_085926283_18.png"   # ì—¬ê¸°ì— ì˜ˆì¸¡í•  ì´ë¯¸ì§€ ê²½ë¡œ ë„£ìœ¼ì„¸ìš”

# res = run_full_infer(TEST_IMG)

# ì´ë¯¸ì§€ ë³´ì—¬ì£¼ê¸°
# pil = Image.open(res["image"]).convert("RGB")
# plt.figure(figsize=(7,7))
# plt.imshow(pil)
# plt.axis("off")
# plt.title(Path(res["image"]).name, fontsize=12)
# plt.show()

# # ê²°ê³¼ ì¶œë ¥
# print("========== ê²°ê³¼ ==========")
# print("ê°ì²´ íƒœê·¸ 3ê°œ:", ", ".join(res["objects"]))
# print("ì¥ì†Œ íƒœê·¸ 3ê°œ:", ", ".join(res["scenes"]))
# print("ë¶„ìœ„ê¸° íƒœê·¸ 3ê°œ:", ", ".join(res["moods"]))
# print("--------------------------------")
# print("ì¹´í…Œê³ ë¦¬ Top-3 (ì„ì¸ ì ìˆ˜):")
# for name, p in res["categories"]:
#     print(f" - {name}: {p*100:.1f}%")
# print("================================")

if __name__ == "__main__":
    TEST_IMG = "./test/KakaoTalk_20251028_085926283_18.png"
    res = run_full_infer(TEST_IMG)

    # ì´ë¯¸ì§€ ë³´ì—¬ì£¼ê¸°
    pil = Image.open(res["image"]).convert("RGB")
    import matplotlib.pyplot as plt
    plt.figure(figsize=(7,7))
    plt.imshow(pil)
    plt.axis("off")
    plt.title(Path(res["image"]).name, fontsize=12)
    plt.show()

    # ê²°ê³¼ ì¶œë ¥
    print("========== ê²°ê³¼ ==========")
    print("ê°ì²´ íƒœê·¸ 3ê°œ:", ", ".join(res["objects"]))
    print("ì¥ì†Œ íƒœê·¸ 3ê°œ:", ", ".join(res["scenes"]))
    print("ë¶„ìœ„ê¸° íƒœê·¸ 3ê°œ:", ", ".join(res["moods"]))
    print("--------------------------------")
    print("ì¹´í…Œê³ ë¦¬ Top-3 (ì„ì¸ ì ìˆ˜):")
    for name, p in res["categories"]:
        print(f" - {name}: {p*100:.1f}%")
    print("================================")
